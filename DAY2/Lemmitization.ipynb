{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lemmitization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Wordnet Lemmatizer with NLTK"
      ],
      "metadata": {
        "id": "tmBZ3eFDTToT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "p94litln1mrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wordnet is an large, freely and publicly available lexical database for the English language aiming to establish structured semantic relationships between words. It offers lemmatization capabilities as well and is one of the earliest and most commonly used lemmatizers. NLTK offers an interface to it, but you have to download it first in order to use it. Follow the below instructions to install nltk and download wordnet"
      ],
      "metadata": {
        "id": "p5RM8ZKICMYK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_QlA-eQsu2y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5585ff96-f3d4-45bc-f38c-d276e7471dd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# How to install and import NLTK\n",
        "# In terminal or prompt:\n",
        "# pip install nltk\n",
        "\n",
        "# # Download Wordnet through NLTK in python console:\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "history , historical - history  - lemmization\n",
        "histori - stemming\n"
      ],
      "metadata": {
        "id": "RgBuNASo2xIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "# Init the Wordnet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize Single Word\n",
        "print(lemmatizer.lemmatize(\"bats\"))\n",
        "\n",
        "\n",
        "print(lemmatizer.lemmatize(\"are\"))\n",
        "\n",
        "\n",
        "print(lemmatizer.lemmatize(\"feet\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mo7FC1CUTOUX",
        "outputId": "bf77cc9c-2352-40e0-eec3-b20e42e0ae9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bat\n",
            "are\n",
            "foot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "# Define the sentence to be lemmatized\n",
        "sentence = \"The striped bats are hanging on their feet for best\"\n",
        "\n",
        "# Tokenize: Split the sentence into words\n",
        "word_list = nltk.word_tokenize(sentence)\n",
        "print(word_list)\n",
        "\n",
        "\n",
        "# Lemmatize list of words and join\n",
        "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
        "print(lemmatized_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYYe9f5FTal8",
        "outputId": "c9759477-cc81-4720-aa3f-871ae204a790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
            "The striped bat are hanging on their foot for best\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part of speech"
      ],
      "metadata": {
        "id": "HOWGndv0UNhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatizer.lemmatize(\"stripes\", 'v'))  \n",
        "\n",
        "\n",
        "print(lemmatizer.lemmatize(\"stripes\", 'n'))  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_G5ceiiTibE",
        "outputId": "f5bf65f9-fa4b-4a90-d6aa-dd7e894bb64f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "strip\n",
            "stripe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wordnet Lemmatizer with appropriate POS tag"
      ],
      "metadata": {
        "id": "VMuzDhSGUayK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe5s6dUhUZ35",
        "outputId": "0274afb0-b0ba-44b1-f0fe-747c944a8017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(nltk.pos_tag(['feet']))\n",
        "#> [('feet', 'NNS')]\n",
        "\n",
        "print(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
        "#> [('The', 'DT'), ('striped', 'JJ'), ('bats', 'NNS'), ('are', 'VBP'), ('hanging', 'VBG'), ('on', 'IN'), ('their', 'PRP$'), ('feet', 'NNS'), ('for', 'IN'), ('best', 'JJS')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UbAIVD-UQ7l",
        "outputId": "2bfefc4c-3149-47a2-c468-3c3fee7c2e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('feet', 'NNS')]\n",
            "[('The', 'DT'), ('striped', 'JJ'), ('bats', 'NNS'), ('are', 'VBP'), ('hanging', 'VBG'), ('on', 'IN'), ('their', 'PRP$'), ('feet', 'NNS'), ('for', 'IN'), ('best', 'JJS')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "# 1. Init Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# 2. Lemmatize Single Word with the appropriate POS tag\n",
        "word = 'feet'\n",
        "print(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
        "\n",
        "# 3. Lemmatize a Sentence with the appropriate POS tag\n",
        "sentence = \"The striped bats are hanging on their feet for best\"\n",
        "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])\n",
        "\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6aFgLMgUkoo",
        "outputId": "b2a23c57-4a08-49a7-8e68-f0134c83f8aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "foot\n",
            "['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using lemminflect"
      ],
      "metadata": {
        "id": "Za4Vct079G5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LemmInflect uses a dictionary approach to lemmatize English words and inflect them into forms specified by a user supplied Universal Dependencies or Penn Treebank tag. The library works with out-of-vocabulary (OOV) words by applying neural network techniques to classify word forms and choose the appropriate morphing rules."
      ],
      "metadata": {
        "id": "bdkaEWGDCUmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lemminflect"
      ],
      "metadata": {
        "id": "TdGxLd7IVNAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99600b28-8e36-4121-9370-874b3af9127e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lemminflect\n",
            "  Downloading lemminflect-0.2.2-py3-none-any.whl (769 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 18.0 MB/s eta 0:00:01\r\u001b[K     |▉                               | 20 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 71 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 81 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 92 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 184 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 194 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 204 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 215 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 225 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 235 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 245 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 256 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 266 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 276 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 286 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 296 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 307 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 317 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 327 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 337 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 348 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 358 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 368 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 378 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 389 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 399 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 409 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 419 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 430 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 440 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 450 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 460 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 471 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 481 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 491 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 501 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 512 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 522 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 532 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 542 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 552 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 563 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 573 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 583 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 593 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 604 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 614 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 624 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 634 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 645 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 655 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 665 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 675 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 686 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 696 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 706 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 716 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 727 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 737 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 747 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 757 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 768 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 769 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lemminflect) (1.19.5)\n",
            "Installing collected packages: lemminflect\n",
            "Successfully installed lemminflect-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lemminflect import getLemma\n",
        "getLemma('watches', upos='VERB')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ib232yD83Xl",
        "outputId": "986f8508-d6d4-4da3-9022-247e5fbcd31e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('watch',)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getLemma('watched', upos='VERB')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyQh8W2O9AOu",
        "outputId": "c56e2818-929b-47ea-c8a9-22a3dc1e9f60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('watch',)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spacy"
      ],
      "metadata": {
        "id": "rKLt8la19xOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy is a relatively new in the space and is billed as an industrial strength NLP engine. It comes with pre-built models that can parse text and compute various NLP related features through one single function call. Ofcourse, it provides the lemma of the word too. Before we begin, let’s install spaCy and download the ‘en’ model."
      ],
      "metadata": {
        "id": "l73Xe3IhCZqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "sentence = \"The striped bats are hanging on their feet for best\"\n",
        "\n",
        "# Parse the sentence using the loaded 'en' model object `nlp`\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Extract the lemma for each token and join\n",
        "\" \".join([token.lemma_ for token in doc])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DFk5Ldjt9D3Z",
        "outputId": "2fc15762-b8e9-4b19-ec3a-339e135f14d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the stripe bat be hang on -PRON- foot for good'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It did all the lemmatizations the Wordnet Lemmatizer supplied with the correct POS tag did. Plus it also lemmatized ‘best’ to ‘good’. Nice! You’d see the -PRON- character coming up whenever spacy detects a pronoun."
      ],
      "metadata": {
        "id": "XsnSJLJY9_Su"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TextBlob Lemmatizer"
      ],
      "metadata": {
        "id": "k-E5qBK4-JmH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TexxtBlob is a powerful, fast and convenient NLP package as well. Using the Word and TextBlob objects, its quite straighforward to parse and lemmatize words and sentences respectively."
      ],
      "metadata": {
        "id": "wigtupXPCk8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y89m6TpX-aRa",
        "outputId": "c79fe4db-4223-46be-9b40-79276e0bf5c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mvar8KPc-hY8",
        "outputId": "a4b65e89-1f8e-4835-ff21-6941b07d9d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install textblob\n",
        "from textblob import TextBlob, Word\n",
        "\n",
        "# Lemmatize a sentence\n",
        "sentence = \"The striped bats are hanging on their feet for best\"\n",
        "sent = TextBlob(sentence)\n",
        "\" \". join([w.lemmatize() for w in sent.words])\n",
        "#> 'The striped bat are hanging on their foot for best'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kpkYm-O--I3G",
        "outputId": "347a0e1a-f28d-4cb3-ebee-bc811ce0403d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The striped bat are hanging on their foot for best'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TextBlob Lemmatizer with appropriate POS tag"
      ],
      "metadata": {
        "id": "AR7VLaPg-5cT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to lemmatize each word with its POS tag\n",
        "def lemmatize_with_postag(sentence):\n",
        "    sent = TextBlob(sentence)\n",
        "    tag_dict = {\"J\": 'a', \n",
        "                \"N\": 'n', \n",
        "                \"V\": 'v', \n",
        "                \"R\": 'r'}\n",
        "    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n",
        "    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
        "    return \" \".join(lemmatized_list)\n",
        "\n",
        "# Lemmatize\n",
        "sentence = \"The striped bats are hanging on their feet for best\"\n",
        "lemmatize_with_postag(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eVyYYgvN9qhd",
        "outputId": "c60148f2-2be6-4820-9511-59f78b1be2b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The striped bat be hang on their foot for best'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thanks "
      ],
      "metadata": {
        "id": "5lKKOpHyCnN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Co9KBQdcApcc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}